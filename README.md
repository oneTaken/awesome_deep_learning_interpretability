# awesome_deep_learning_interpretability
深度学习近年来关于模型解释性的相关论文。

按引用次数排序可见[引用排序](./sort_cite.md)

159篇论文pdf(有2篇需要上scihub找)上传到[腾讯微云](https://share.weiyun.com/5ddB0EQ)。

不定期更新。

|Year|Publication|Paper|Citation|code|
|:---:|:---:|:---:|:---:|:---:|
|2020|CVPR|[Explaining Knowledge Distillation by Quantifying the Knowledge](https://arxiv.org/pdf/2003.03622.pdf)|3|
|2020|CVPR|[High-frequency Component Helps Explain the Generalization of Convolutional Neural Networks](https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_High-Frequency_Component_Helps_Explain_the_Generalization_of_Convolutional_Neural_Networks_CVPR_2020_paper.pdf)|16|
|2020|CVPRW|[Score-CAM: Score-Weighted Visual Explanations for Convolutional Neural Networks](https://openaccess.thecvf.com/content_CVPRW_2020/papers/w1/Wang_Score-CAM_Score-Weighted_Visual_Explanations_for_Convolutional_Neural_Networks_CVPRW_2020_paper.pdf)|7|[Pytorch](https://github.com/haofanwang/Score-CAM)
|2020|ICLR|[Knowledge consistency between neural networks and beyond](https://arxiv.org/pdf/1908.01581.pdf)|3|
|2020|ICLR|[Interpretable Complex-Valued Neural Networks for Privacy Protection](https://arxiv.org/pdf/1901.09546.pdf)|2|
|2019|AI|[Explanation in artificial intelligence: Insights from the social sciences](https://arxiv.org/pdf/1706.07269.pdf)|662|
|2019|NMI|[Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead](https://arxiv.org/pdf/1811.10154.pdf)|389|
|2019|NeurIPS|[Can you trust your model's uncertainty? Evaluating predictive uncertainty under dataset shift](https://papers.nips.cc/paper/9547-can-you-trust-your-models-uncertainty-evaluating-predictive-uncertainty-under-dataset-shift.pdf)|136|-|
|2019|NeurIPS|[This looks like that: deep learning for interpretable image recognition](http://papers.nips.cc/paper/9095-this-looks-like-that-deep-learning-for-interpretable-image-recognition.pdf)|80|[Pytorch](https://github.com/cfchen-duke/ProtoPNet)|
|2019|NeurIPS|[A benchmark for interpretability methods in deep neural networks](https://papers.nips.cc/paper/9167-a-benchmark-for-interpretability-methods-in-deep-neural-networks.pdf)|28|
|2019|NeurIPS|[Full-gradient representation for neural network visualization](http://papers.nips.cc/paper/8666-full-gradient-representation-for-neural-network-visualization.pdf)|7|
|2019|NeurIPS|[On the (In) fidelity and Sensitivity of Explanations](https://papers.nips.cc/paper/9278-on-the-infidelity-and-sensitivity-of-explanations.pdf)|13|
|2019|NeurIPS|[Towards Automatic Concept-based Explanations](http://papers.nips.cc/paper/9126-towards-automatic-concept-based-explanations.pdf)|25|[Tensorflow](https://github.com/amiratag/ACE)|
|2019|NeurIPS|[CXPlain: Causal explanations for model interpretation under uncertainty](http://papers.nips.cc/paper/9211-cxplain-causal-explanations-for-model-interpretation-under-uncertainty.pdf)|12|
|2019|CVPR|[Interpreting CNNs via Decision Trees](http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Interpreting_CNNs_via_Decision_Trees_CVPR_2019_paper.pdf)|85|
|2019|CVPR|[From Recognition to Cognition: Visual Commonsense Reasoning](http://openaccess.thecvf.com/content_CVPR_2019/papers/Zellers_From_Recognition_to_Cognition_Visual_Commonsense_Reasoning_CVPR_2019_paper.pdf)|97|[Pytorch](https://github.com/rowanz/r2c)|
|2019|CVPR|[Attention branch network: Learning of attention mechanism for visual explanation](http://openaccess.thecvf.com/content_CVPR_2019/papers/Fukui_Attention_Branch_Network_Learning_of_Attention_Mechanism_for_Visual_Explanation_CVPR_2019_paper.pdf)|39|
|2019|CVPR|[Interpretable and fine-grained visual explanations for convolutional neural networks](http://openaccess.thecvf.com/content_CVPR_2019/papers/Wagner_Interpretable_and_Fine-Grained_Visual_Explanations_for_Convolutional_Neural_Networks_CVPR_2019_paper.pdf)|18|
|2019|CVPR|[Learning to Explain with Complemental Examples](http://openaccess.thecvf.com/content_CVPR_2019/papers/Kanehira_Learning_to_Explain_With_Complemental_Examples_CVPR_2019_paper.pdf)|12|
|2019|CVPR|[Revealing Scenes by Inverting Structure from Motion Reconstructions](http://openaccess.thecvf.com/content_CVPR_2019/papers/Pittaluga_Revealing_Scenes_by_Inverting_Structure_From_Motion_Reconstructions_CVPR_2019_paper.pdf)|20|[Tensorflow](https://github.com/francescopittaluga/invsfm)|
|2019|CVPR|[Multimodal Explanations by Predicting Counterfactuality in Videos](http://openaccess.thecvf.com/content_CVPR_2019/papers/Kanehira_Multimodal_Explanations_by_Predicting_Counterfactuality_in_Videos_CVPR_2019_paper.pdf)|4|
|2019|CVPR|[Visualizing the Resilience of Deep Convolutional Network Interpretations](http://openaccess.thecvf.com/content_CVPRW_2019/papers/Explainable%20AI/Vasu_Visualizing_the_Resilience_of_Deep_Convolutional_Network_Interpretations_CVPRW_2019_paper.pdf)|1|
|2019|ICCV|[U-CAM: Visual Explanation using Uncertainty based Class Activation Maps](http://openaccess.thecvf.com/content_ICCV_2019/papers/Patro_U-CAM_Visual_Explanation_Using_Uncertainty_Based_Class_Activation_Maps_ICCV_2019_paper.pdf)|10|
|2019|ICCV|[Towards Interpretable Face Recognition](https://arxiv.org/pdf/1805.00611.pdf)|7|
|2019|ICCV|[Taking a HINT: Leveraging Explanations to Make Vision and Language Models More Grounded](http://openaccess.thecvf.com/content_ICCV_2019/papers/Selvaraju_Taking_a_HINT_Leveraging_Explanations_to_Make_Vision_and_Language_ICCV_2019_paper.pdf)|28|
|2019|ICCV|[Understanding Deep Networks via Extremal Perturbations and Smooth Masks](http://openaccess.thecvf.com/content_ICCV_2019/papers/Fong_Understanding_Deep_Networks_via_Extremal_Perturbations_and_Smooth_Masks_ICCV_2019_paper.pdf)|17|[Pytorch](https://github.com/facebookresearch/TorchRay)|
|2019|ICCV|[Explaining Neural Networks Semantically and Quantitatively](http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Explaining_Neural_Networks_Semantically_and_Quantitatively_ICCV_2019_paper.pdf)|6|
|2019|ICLR|[Hierarchical interpretations for neural network predictions](https://arxiv.org/pdf/1806.05337.pdf)|24|[Pytorch](https://github.com/csinva/hierarchical-dnn-interpretations)|
|2019|ICLR|[How Important Is a Neuron?](https://arxiv.org/pdf/1805.12233.pdf)|32|
|2019|ICLR|[Visual Explanation by Interpretation: Improving Visual Feedback Capabilities of Deep Neural Networks](https://arxiv.org/pdf/1712.06302.pdf)|13|
|2018|ICML|[Extracting Automata from Recurrent Neural Networks Using Queries and Counterexamples](https://arxiv.org/pdf/1711.09576.pdf)|71|[Pytorch](https://github.com/tech-srl/lstar_extraction)|
|2019|ICML|[Towards A Deep and Unified Understanding of Deep Neural Models in NLP](http://proceedings.mlr.press/v97/guan19a/guan19a.pdf)|15|[Pytorch](https://github.com/icml2019paper2428/Towards-A-Deep-and-Unified-Understanding-of-Deep-Neural-Models-in-NLP)|
|2019|ICAIS|[Interpreting black box predictions using fisher kernels](https://arxiv.org/pdf/1810.10118.pdf)|24|
|2019|ACMFAT|[Explaining explanations in AI](https://s3.amazonaws.com/academia.edu.documents/57692790/Mittelstadt__Russell_and_Wachter_-_2019_-_Explaining_Explanations_in_AI.pdf?response-content-disposition=inline%3B%20filename%3DExplaining_Explanations_in_AI.pdf&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=ASIATUSBJ6BAJW2TMFXG%2F20200528%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200528T052420Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEHUaCXVzLWVhc3QtMSJIMEYCIQDCCKV%2FpUmJZHn03yzTquQ%2FNMtaXW%2FC63WPmQd%2FhImmYAIhAMelsFwqb9IfV4W2xlfL%2FHk4qeovouLdYbXKf%2B1%2FMwvyKr0DCM7%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQABoMMjUwMzE4ODExMjAwIgytA%2BM6OWOGN4XLrlUqkQN2f8ywZT0AEUzKdbVDyGvZN%2B1repdgXrfgT2rAJiGacTK8IRCoyECvRgcgS%2BWJWYpjS7CjoL%2BlTm1c%2BWDWdo%2FYnVM0U6shk9OQivK089W064ZR64AQCCkBDutI3vYhP%2BOJ8AtEUDE%2B7W5EWVQ4zeUDG4ryxzdomFnrHpzA5fp05qWrOmPS0vd%2FFabC%2FPKXO34bpfgyRzz3PHrIsUC2%2BPB0EAo7CPKS0Ux%2FlxmiIOYOIj5u1ZKoP8NVLgOfueQe7%2F%2F3VJUnUXSAIsAThszDTnbi0AJEjvNvUHjm8E%2F7zqBApJ6YVd39NkKl8%2BTE7MRwKuITAOIq8jsyta%2FcmIY5igpHpVCkYcG395rHfScDu3CODXIAcKRLX%2F7brNz%2FRHuGhddK3Q2XuGTjQaeLTEYTmTj2e7VDDmEOt%2BpxvXx7UaImPakzpVZ1Ks6APy1JHupKgBhM6JJkeFprlK62e4sf09wqwxk9KsJSot3TMLVwM63yGr7VmXdg61ETsg0D%2BO1DOnnMprsFhEkb%2Bt%2FpCVafebolsjCN%2Frz2BTrqAZiqy6Obte6J%2BeHJ5bzB1sy1oF%2Fi7ueF56nd1C9ObB%2FXLx930j8wqmakO%2FnoaUiYM6gHh1jZbl8cCeLr8Xu0YSGecpe1J5HECU0A5%2Fq68zoBDfyY6UGNZJ%2B87Br6crqpfaHFkP5g4zXvuN2%2F0fp6S9m2iuSRBr%2B%2Bh2Z1rXmvb3Vequ2qgqeJBS2nHOX8pLp2LhJsVMqdl218jeQDsjYnbxJKq86peVGr66Cuv7TmNiimVl0c0dPr1jgjr25N9hvMnpX83n2Xa%2Fz%2BHUmaYfwFLrD0YLkUWaS2Khcpm0%2BwvrcYsQEyOmYkVG8x5Q%3D%3D&X-Amz-Signature=4fcca52f4ae92746068ea2164846aca05c2bb44e04c1330947ba70f75e676171)|119|
|2019|AAAI|[Interpretation of neural networks is fragile](https://machine-learning-and-security.github.io/papers/mlsec17_paper_18.pdf)|130|[Tensorflow](https://github.com/amiratag/InterpretationFragility)|
|2019|AAAI|[Classifier-agnostic saliency map extraction](https://arxiv.org/pdf/1805.08249.pdf)|8|
|2019|AAAI|[Can You Explain That? Lucid Explanations Help Human-AI Collaborative Image Retrieval](https://arxiv.org/pdf/1904.03285.pdf)|1|
|2019|AAAIW|[Unsupervised Learning of Neural Networks to Explain Neural Networks](https://arxiv.org/pdf/1805.07468.pdf)|10|
|2019|AAAIW|[Network Transplanting](https://arxiv.org/pdf/1804.10272.pdf)|4|
|2019|CSUR|[A Survey of Methods for Explaining Black Box Models](https://kdd.isti.cnr.it/sites/kdd.isti.cnr.it/files/csur2018survey.pdf)|655|
|2019|JVCIR|[Interpretable convolutional neural networks via feedforward design](https://arxiv.org/pdf/1810.02786)|31|[Keras](https://github.com/davidsonic/Interpretable_CNNs_via_Feedforward_Design)|
|2019|ExplainAI|[The (Un)reliability of saliency methods](https://arxiv.org/pdf/1711.00867.pdf)|128|
|2019|ACL|[Attention is not Explanation](https://arxiv.org/pdf/1902.10186.pdf)|157|
|2019|EMNLP|[Attention is not not Explanation](https://arxiv.org/pdf/1908.04626.pdf)|57|
|2019|arxiv|[Attention Interpretability Across NLP Tasks](https://arxiv.org/pdf/1909.11218.pdf)|16|
|2019|arxiv|[Interpretable CNNs](https://arxiv.org/pdf/1901.02413.pdf)|2|
|2018|ICLR|[Towards better understanding of gradient-based attribution methods for deep neural networks](https://arxiv.org/pdf/1711.06104.pdf)|245|
|2018|ICLR|[Learning how to explain neural networks: PatternNet and PatternAttribution](https://arxiv.org/pdf/1705.05598.pdf)|143|
|2018|ICLR|[On the importance of single directions for generalization](https://arxiv.org/pdf/1803.06959.pdf)|134|[Pytorch](https://github.com/1Konny/class_selectivity_index)|
|2018|ICLR|[Detecting statistical interactions from neural network weights](https://arxiv.org/pdf/1705.04977.pdf)|56|[Pytorch](https://github.com/mtsang/neural-interaction-detection)|
|2018|ICLR|[Interpretable counting for visual question answering](https://arxiv.org/pdf/1712.08697.pdf)|29|[Pytorch](https://github.com/sanyam5/irlc-vqa-counting)|
|2018|CVPR|[Interpretable Convolutional Neural Networks](http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Interpretable_Convolutional_Neural_CVPR_2018_paper.pdf)|250|
|2018|CVPR|[Tell me where to look: Guided attention inference network](http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Tell_Me_Where_CVPR_2018_paper.pdf)|134|[Chainer](https://github.com/alokwhitewolf/Guided-Attention-Inference-Network)|
|2018|CVPR|[Multimodal Explanations: Justifying Decisions and Pointing to the Evidence](http://openaccess.thecvf.com/content_cvpr_2018/papers/Park_Multimodal_Explanations_Justifying_CVPR_2018_paper.pdf)|126|[Caffe](https://github.com/Seth-Park/MultimodalExplanations)|
|2018|CVPR|[Transparency by design: Closing the gap between performance and interpretability in visual reasoning](http://openaccess.thecvf.com/content_cvpr_2018/papers/Mascharka_Transparency_by_Design_CVPR_2018_paper.pdf)|79|[Pytorch](https://github.com/davidmascharka/tbd-nets)|
|2018|CVPR|[Net2vec: Quantifying and explaining how concepts are encoded by filters in deep neural networks](http://openaccess.thecvf.com/content_cvpr_2018/papers/Fong_Net2Vec_Quantifying_and_CVPR_2018_paper.pdf)|60|
|2018|CVPR|[What have we learned from deep representations for action recognition?](http://openaccess.thecvf.com/content_cvpr_2018/papers/Feichtenhofer_What_Have_We_CVPR_2018_paper.pdf)|30|
|2018|CVPR|[Learning to Act Properly: Predicting and Explaining Affordances from Images](http://openaccess.thecvf.com/content_cvpr_2018/papers/Chuang_Learning_to_Act_CVPR_2018_paper.pdf)|24|
|2018|CVPR|[Teaching Categories to Human Learners with Visual Explanations](http://openaccess.thecvf.com/content_cvpr_2018/papers/Aodha_Teaching_Categories_to_CVPR_2018_paper.pdf)|20|[Pytorch](https://github.com/macaodha/explain_teach)|
|2018|CVPR|[What do deep networks like to see?](http://openaccess.thecvf.com/content_cvpr_2018/papers/Palacio_What_Do_Deep_CVPR_2018_paper.pdf)|19|
|2018|CVPR|[Interpret Neural Networks by Identifying Critical Data Routing Paths](http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Interpret_Neural_Networks_CVPR_2018_paper.pdf)|13|[Tensorflow](https://github.com/lidongyue12138/CriticalPathPruning)|
|2018|ECCV|[Deep clustering for unsupervised learning of visual features](http://openaccess.thecvf.com/content_ECCV_2018/papers/Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper.pdf)|382|[Pytorch](https://github.com/asanakoy/deep_clustering)|
|2018|ECCV|[Explainable neural computation via stack neural module networks](http://openaccess.thecvf.com/content_ECCV_2018/papers/Ronghang_Hu_Explainable_Neural_Computation_ECCV_2018_paper.pdf)|55|[Tensorflow](https://github.com/ronghanghu/snmn)|
|2018|ECCV|[Grounding visual explanations](http://openaccess.thecvf.com/content_ECCV_2018/papers/Lisa_Anne_Hendricks_Grounding_Visual_Explanations_ECCV_2018_paper.pdf)|44|
|2018|ECCV|[Textual explanations for self-driving vehicles](http://openaccess.thecvf.com/content_ECCV_2018/papers/Jinkyu_Kim_Textual_Explanations_for_ECCV_2018_paper.pdf)|59|
|2018|ECCV|[Interpretable basis decomposition for visual explanation](http://openaccess.thecvf.com/content_ECCV_2018/papers/Antonio_Torralba_Interpretable_Basis_Decomposition_ECCV_2018_paper.pdf)|51|[Pytorch](https://github.com/CSAILVision/IBD)|
|2018|ECCV|[Convnets and imagenet beyond accuracy: Understanding mistakes and uncovering biases](http://openaccess.thecvf.com/content_ECCV_2018/papers/Pierre_Stock_ConvNets_and_ImageNet_ECCV_2018_paper.pdf)|36|
|2018|ECCV|[Vqa-e: Explaining, elaborating, and enhancing your answers for visual questions](http://openaccess.thecvf.com/content_ECCV_2018/papers/Qing_Li_VQA-E_Explaining_Elaborating_ECCV_2018_paper.pdf)|20|
|2018|ECCV|[Choose Your Neuron: Incorporating Domain Knowledge through Neuron-Importance](http://openaccess.thecvf.com/content_ECCV_2018/papers/Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper.pdf)|16|[Pytorch](https://github.com/ramprs/neuron-importance-zsl)|
|2018|ECCV|[Diverse feature visualizations reveal invariances in early layers of deep neural networks](http://openaccess.thecvf.com/content_ECCV_2018/papers/Santiago_Cadena_Diverse_feature_visualizations_ECCV_2018_paper.pdf)|9|[Tensorflow](https://github.com/sacadena/diverse_feature_vis)|
|2018|ECCV|[ExplainGAN: Model Explanation via Decision Boundary Crossing Transformations](http://openaccess.thecvf.com/content_ECCV_2018/papers/Nathan_Silberman_ExplainGAN_Model_Explanation_ECCV_2018_paper.pdf)|6|
|2018|ICML|[Interpretability beyond feature attribution: Quantitative testing with concept activation vectors](https://arxiv.org/pdf/1711.11279.pdf)|214|[Tensorflow](https://github.com/fursovia/tcav_nlp)|
|2018|ICML|[Learning to explain: An information-theoretic perspective on model interpretation](https://arxiv.org/pdf/1802.07814.pdf)|117|
|2018|ACL|[Did the Model Understand the Question?](https://arxiv.org/pdf/1805.05492.pdf)|63|[Tensorflow](https://github.com/pramodkaushik/acl18_results)|
|2018|FITEE|[Visual interpretability for deep learning: a survey](https://arxiv.org/pdf/1802.00614)|243|
|2018|NeurIPS|[Sanity Checks for Saliency Maps](http://papers.nips.cc/paper/8160-sanity-checks-for-saliency-maps.pdf)|249|
|2018|NeurIPS|[Explanations based on the missing: Towards contrastive explanations with pertinent negatives](http://papers.nips.cc/paper/7340-explanations-based-on-the-missing-towards-contrastive-explanations-with-pertinent-negatives.pdf)|79|[Tensorflow](https://github.com/IBM/Contrastive-Explanation-Method)|
|2018|NeurIPS|[Towards robust interpretability with self-explaining neural networks](http://papers.nips.cc/paper/8003-towards-robust-interpretability-with-self-explaining-neural-networks.pdf)|145|[Pytorch](https://github.com/raj-shah/senn)|
|2018|NeurIPS|[Attacks meet interpretability: Attribute-steered detection of adversarial samples](https://papers.nips.cc/paper/7998-attacks-meet-interpretability-attribute-steered-detection-of-adversarial-samples.pdf)|55|
|2018|NeurIPS|[DeepPINK: reproducible feature selection in deep neural networks](https://papers.nips.cc/paper/8085-deeppink-reproducible-feature-selection-in-deep-neural-networks.pdf)|30|[Keras](https://github.com/younglululu/DeepPINK)|
|2018|NeurIPS|[Representer point selection for explaining deep neural networks](https://papers.nips.cc/paper/8141-representer-point-selection-for-explaining-deep-neural-networks.pdf)|30|[Tensorflow](https://github.com/chihkuanyeh/Representer_Point_Selection)|
|2018|NeurIPS Workshop|[Interpretable convolutional filters with sincNet](https://arxiv.org/pdf/1811.09725)|37|
|2018|AAAI|[Anchors: High-precision model-agnostic explanations](https://dm-gatech.github.io/CS8803-Fall2018-DML-Papers/anchors.pdf)|366|
|2018|AAAI|[Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients](https://asross.github.io/publications/RossDoshiVelez2018.pdf)|178|[Tensorflow](https://github.com/dtak/adversarial-robustness-public)|
|2018|AAAI|[Deep learning for case-based reasoning through prototypes: A neural network that explains its predictions](https://arxiv.org/pdf/1710.04806.pdf)|102|[Tensorflow](https://github.com/OscarcarLi/PrototypeDL)|
|2018|AAAI|[Interpreting CNN Knowledge via an Explanatory Graph](https://arxiv.org/pdf/1708.01785.pdf)|79|[Matlab](https://github.com/zqs1022/explanatoryGraph)|
|2018|AAAI|[Examining CNN Representations with respect to Dataset Bias](http://www.stat.ucla.edu/~sczhu/papers/Conf_2018/AAAI_2018_DNN_Learning_Bias.pdf)|37|
|2018|WACV|[Grad-cam++: Generalized gradient-based visual explanations for deep convolutional networks](https://www.researchgate.net/profile/Aditya_Chattopadhyay2/publication/320727679_Grad-CAM_Generalized_Gradient-based_Visual_Explanations_for_Deep_Convolutional_Networks/links/5a3aa2e5a6fdcc3889bd04cb/Grad-CAM-Generalized-Gradient-based-Visual-Explanations-for-Deep-Convolutional-Networks.pdf)|174|
|2018|IJCV|[Top-down neural attention by excitation backprop](https://arxiv.org/pdf/1608.00507)|329|
|2018|TPAMI|[Interpreting deep visual representations via network dissection](https://arxiv.org/pdf/1711.05611)|87|
|2018|DSP|[Methods for interpreting and understanding deep neural networks](http://iphome.hhi.de/samek/pdf/MonDSP18.pdf)|713|
|2018|Access|[Peeking inside the black-box: A survey on Explainable Artificial Intelligence (XAI)](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8466590)|390|
|2018|JAIR|[Learning Explanatory Rules from Noisy Data](https://www.ijcai.org/Proceedings/2018/0792.pdf)|155|[Tensorflow](https://github.com/ai-systems/DILP-Core)|
|2018|MIPRO|[Explainable artificial intelligence: A survey](https://www.researchgate.net/profile/Mario_Brcic/publication/325398586_Explainable_Artificial_Intelligence_A_Survey/links/5b0bec90a6fdcc8c2534d673/Explainable-Artificial-Intelligence-A-Survey.pdf)|108|
|2018|BMVC|[Rise: Randomized input sampling for explanation of black-box models](https://arxiv.org/pdf/1806.07421.pdf)|85|
|2018|arxiv|[Distill-and-Compare: Auditing Black-Box Models Using Transparent Model Distillation](https://arxiv.org/pdf/1710.06169.pdf)|30|
|2018|arxiv|[Manipulating and measuring model interpretability](https://arxiv.org/pdf/1802.07810.pdf)|133|
|2018|arxiv|[How convolutional neural network see the world-A survey of convolutional neural network visualization methods](https://arxiv.org/pdf/1804.11191.pdf)|45|
|2018|arxiv|[Revisiting the importance of individual units in cnns via ablation](https://arxiv.org/pdf/1806.02891.pdf)|43|
|2018|arxiv|[Computationally Efficient Measures of Internal Neuron Importance](https://arxiv.org/pdf/1807.09946.pdf)|1|
|2017|ICML|[Understanding Black-box Predictions via Influence Functions](https://dm-gatech.github.io/CS8803-Fall2018-DML-Papers/influence-functions.pdf)|767|[Pytorch](https://github.com/nimarb/pytorch_influence_functions)|
|2017|ICML|[Axiomatic attribution for deep networks](https://mit6874.github.io/assets/misc/sundararajan.pdf)|755|[Keras](https://github.com/hiranumn/IntegratedGradients)|
|2017|ICML|[Learning Important Features Through Propagating Activation Differences](https://mit6874.github.io/assets/misc/shrikumar.pdf)|655|
|2017|ICLR|[Visualizing deep neural network decisions: Prediction difference analysis](https://arxiv.org/pdf/1702.04595.pdf)|271|[Caffe](https://github.com/lmzintgraf/DeepVis-PredDiff)|
|2017|ICLR|[Exploring LOTS in Deep Neural Networks](https://openreview.net/pdf?id=SkCILwqex)|27|
|2017|NeurIPS|[A Unified Approach to Interpreting Model Predictions](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf)|1411|
|2017|NeurIPS|[Real time image saliency for black box classifiers](https://papers.nips.cc/paper/7272-real-time-image-saliency-for-black-box-classifiers.pdf)|161|[Pytorch](https://github.com/karanchahal/SaliencyMapper)|
|2017|NeurIPS|[SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability](http://papers.nips.cc/paper/7188-svcca-singular-vector-canonical-correlation-analysis-for-deep-learning-dynamics-and-interpretability.pdf)|160|
|2017|CVPR|[Mining Object Parts from CNNs via Active Question-Answering](http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_Mining_Object_Parts_CVPR_2017_paper.pdf)|20|
|2017|CVPR|[Network dissection: Quantifying interpretability of deep visual representations](http://openaccess.thecvf.com/content_cvpr_2017/papers/Bau_Network_Dissection_Quantifying_CVPR_2017_paper.pdf)|540|
|2017|CVPR|[Improving Interpretability of Deep Neural Networks with Semantic Information](http://openaccess.thecvf.com/content_cvpr_2017/papers/Dong_Improving_Interpretability_of_CVPR_2017_paper.pdf)|56|
|2017|CVPR|[MDNet: A Semantically and Visually Interpretable Medical Image Diagnosis Network](http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_MDNet_A_Semantically_CVPR_2017_paper.pdf)|129|[Torch](https://github.com/zizhaozhang/mdnet-cvpr2017)|
|2017|CVPR|[Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering](http://openaccess.thecvf.com/content_cvpr_2017/papers/Goyal_Making_the_v_CVPR_2017_paper.pdf)|582|
|2017|CVPR|[Knowing when to look: Adaptive attention via a visual sentinel for image captioning](http://openaccess.thecvf.com/content_cvpr_2017/papers/Lu_Knowing_When_to_CVPR_2017_paper.pdf)|620|[Torch](https://github.com/jiasenlu/AdaptiveAttention)|
|2017|CVPRW|[Interpretable 3d human action analysis with temporal convolutional networks](http://openaccess.thecvf.com/content_cvpr_2017_workshops/w20/papers/Kim_Interpretable_3D_Human_CVPR_2017_paper.pdf)|163|
|2017|ICCV|[Grad-cam: Visual explanations from deep networks via gradient-based localization](http://openaccess.thecvf.com/content_ICCV_2017/papers/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.pdf)|2444|[Pytorch](https://github.com/leftthomas/GradCAM)|
|2017|ICCV|[Interpretable Explanations of Black Boxes by Meaningful Perturbation](http://openaccess.thecvf.com/content_ICCV_2017/papers/Fong_Interpretable_Explanations_of_ICCV_2017_paper.pdf)|419|[Pytorch](https://github.com/jacobgil/pytorch-explain-black-box)|
|2017|ICCV|[Interpretable Learning for Self-Driving Cars by Visualizing Causal Attention](http://openaccess.thecvf.com/content_ICCV_2017/papers/Kim_Interpretable_Learning_for_ICCV_2017_paper.pdf)|114|
|2017|ICCV|[Understanding and comparing deep neural networks for age and gender classification](http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w23/Lapuschkin_Understanding_and_Comparing_ICCV_2017_paper.pdf)|52|
|2017|ICCV|[Learning to disambiguate by asking discriminative questions](http://openaccess.thecvf.com/content_ICCV_2017/papers/Li_Learning_to_Disambiguate_ICCV_2017_paper.pdf)|12|
|2017|IJCAI|[Right for the right reasons: Training differentiable models by constraining their explanations](https://arxiv.org/pdf/1703.03717.pdf)|149|
|2017|IJCAI|[Understanding and improving convolutional neural networks via concatenated rectified linear units](http://www.jmlr.org/proceedings/papers/v48/shang16.pdf)|276|[Caffe](https://github.com/chakkritte/CReLU)|
|2017|AAAI|[Growing Interpretable Part Graphs on ConvNets via Multi-Shot Learning](https://arxiv.org/pdf/1611.04246.pdf)|37|[Matlab](https://github.com/zqs1022/partGraphForCNN)|
|2017|ACL|[Visualizing and Understanding Neural Machine Translation](https://www.aclweb.org/anthology/P17-1106.pdf)|92|
|2017|EMNLP|[A causal framework for explaining the predictions of black-box sequence-to-sequence models](https://arxiv.org/pdf/1707.01943.pdf)|92|
|2017|CVPR Workshop|[Looking under the hood: Deep neural network visualization to interpret whole-slide image analysis outcomes for colorectal polyps](http://openaccess.thecvf.com/content_cvpr_2017_workshops/w8/papers/Korbar_Looking_Under_the_CVPR_2017_paper.pdf)|21|
|2017|survey|[Interpretability of deep learning models: a survey of results](https://discovery.ucl.ac.uk/id/eprint/10059575/1/Chakraborty_Interpretability%20of%20deep%20learning%20models.pdf)|99|
|2017|arxiv|[SmoothGrad: removing noise by adding noise](https://arxiv.org/pdf/1706.03825.pdf)|356|
|2017|arxiv|[Interpretable & explorable approximations of black box models](https://arxiv.org/pdf/1707.01154.pdf)|115|
|2017|arxiv|[Distilling a neural network into a soft decision tree](https://arxiv.org/pdf/1711.09784.pdf)|188|[Pytorch](https://github.com/kimhc6028/soft-decision-tree)|
|2017|arxiv|[Towards interpretable deep neural networks by leveraging adversarial examples](https://arxiv.org/pdf/1708.05493.pdf)|54|
|2017|arxiv|[Explainable artificial intelligence: Understanding, visualizing and interpreting deep learning models](https://arxiv.org/pdf/1708.08296.pdf)|383|
|2017|arxiv|[Contextual Explanation Networks](https://arxiv.org/pdf/1705.10301.pdf)|35|[Pytorch](https://github.com/alshedivat/cen)|
|2017|arxiv|[Challenges for transparency](https://arxiv.org/pdf/1708.01870.pdf)|83|
|2017|ACMSOPP|[Deepxplore: Automated whitebox testing of deep learning systems](https://machine-learning-and-security.github.io/papers/mlsec17_paper_1.pdf)|431|
|2017|CEURW|[What does explainable AI really mean? A new conceptualization of perspectives](https://arxiv.org/pdf/1710.00794.pdf)|117|
|2017|TVCG|[ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models](https://arxiv.org/pdf/1704.01942.pdf)|158|
|2016|NeurIPS|[Synthesizing the preferred inputs for neurons in neural networks via deep generator networks](http://papers.nips.cc/paper/6519-synthesizing-the-preferred-inputs-for-neurons-in-neural-networks-via-deep-generator-networks.pdf)|321|[Caffe](https://github.com/Evolving-AI-Lab/synthesizing)|
|2016|NeurIPS|[Understanding the effective receptive field in deep convolutional neural networks](https://papers.nips.cc/paper/6203-understanding-the-effective-receptive-field-in-deep-convolutional-neural-networks.pdf)|436|
|2016|CVPR|[Inverting Visual Representations with Convolutional Networks](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Dosovitskiy_Inverting_Visual_Representations_CVPR_2016_paper.pdf)|336|
|2016|CVPR|[Visualizing and Understanding Deep Texture Representations](http://openaccess.thecvf.com/content_cvpr_2016/papers/Lin_Visualizing_and_Understanding_CVPR_2016_paper.pdf)|98|
|2016|CVPR|[Analyzing Classifiers: Fisher Vectors and Deep Neural Networks](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Bach_Analyzing_Classifiers_Fisher_CVPR_2016_paper.pdf)|110|
|2016|ECCV|[Generating Visual Explanations](https://arxiv.org/pdf/1603.08507)|303|[Caffe](https://github.com/LisaAnne/ECCV2016)|
|2016|ECCV|[Design of kernels in convolutional neural networks for image classification](https://arxiv.org/pdf/1511.09231.pdf)|14|
|2016|ICML|[Understanding and improving convolutional neural networks via concatenated rectified linear units](http://www.jmlr.org/proceedings/papers/v48/shang16.pdf)|276|
|2016|ICML|[Visualizing and comparing AlexNet and VGG using deconvolutional layers](https://icmlviz.github.io/icmlviz2016/assets/papers/4.pdf)|41|
|2016|EMNLP|[Rationalizing Neural Predictions](https://arxiv.org/pdf/1606.04155)|355|[Pytorch](https://github.com/zhaopku/Rationale-Torch)|
|2016|IJCV|[Visualizing deep convolutional neural networks using natural pre-images](https://arxiv.org/pdf/1512.02017)|281|[Matlab](https://github.com/aravindhm/nnpreimage)|
|2016|IJCV|[Visualizing Object Detection Features](https://arxiv.org/pdf/1502.05461.pdf)|27|[Caffe](https://github.com/cvondrick/ihog)|
|2016|KDD|[Why should i trust you?: Explaining the predictions of any classifier](https://chu-data-lab.github.io/CS8803Fall2018/CS8803-Fall2018-DML-Papers/lime.pdf)|3511|
|2016|TVCG|[Visualizing the hidden activity of artificial neural networks](https://www.researchgate.net/profile/Samuel_Fadel/publication/306049229_Visualizing_the_Hidden_Activity_of_Artificial_Neural_Networks/links/5b13ffa7aca2723d9980083c/Visualizing-the-Hidden-Activity-of-Artificial-Neural-Networks.pdf)|170|
|2016|TVCG|[Towards better analysis of deep convolutional neural networks](https://arxiv.org/pdf/1604.07043.pdf)|241|
|2016|NAACL|[Visualizing and understanding neural models in nlp](https://arxiv.org/pdf/1506.01066)|364|[Torch](https://github.com/jiweil/Visualizing-and-Understanding-Neural-Models-in-NLP)|
|2016|arxiv|[Understanding neural networks through representation erasure](https://arxiv.org/pdf/1612.08220.pdf))|198|
|2016|arxiv|[Grad-CAM: Why did you say that?](https://arxiv.org/pdf/1611.07450.pdf)|130|
|2016|arxiv|[Investigating the influence of noise and distractors on the interpretation of neural networks](https://arxiv.org/pdf/1611.07270.pdf)|41|
|2016|arxiv|[Attentive Explanations: Justifying Decisions and Pointing to the Evidence](https://arxiv.org/pdf/1612.04757)|54|
|2016|arxiv|[The Mythos of Model Interpretability](http://www.zacklipton.com/media/papers/mythos_model_interpretability_lipton2016.pdf)|1368|
|2016|arxiv|[Multifaceted feature visualization: Uncovering the different types of features learned by each neuron in deep neural networks](https://arxiv.org/pdf/1602.03616)|161|
|2015|ICLR|[Striving for Simplicity: The All Convolutional Net](https://arxiv.org/pdf/1412.6806.pdf)|2268|[Pytorch](https://github.com/StefOe/all-conv-pytorch)|
|2015|CVPR|[Understanding deep image representations by inverting them](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Mahendran_Understanding_Deep_Image_2015_CVPR_paper.pdf)|1129|[Matlab](https://github.com/aravindhm/deep-goggle)|
|2015|ICCV|[Understanding deep features with computer-generated imagery](http://openaccess.thecvf.com/content_iccv_2015/papers/Aubry_Understanding_Deep_Features_ICCV_2015_paper.pdf)|109|[Caffe](https://github.com/mathieuaubry/features_analysis)|
|2015|ICML Workshop|[Understanding Neural Networks Through Deep Visualization](https://arxiv.org/pdf/1506.06579.pdf)|1216|[Tensorflow](https://github.com/jiye-ML/Visualizing-and-Understanding-Convolutional-Networks)|
|2015|AAS|[Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model](https://projecteuclid.org/download/pdfview_1/euclid.aoas/1446488742)|385|
|2014|ECCV|[Visualizing and Understanding Convolutional Networks](https://arxiv.org/pdf/1311.2901.pdf)|9873|[Pytorch](https://github.com/huybery/VisualizingCNN)|
|2014|ICLR|[Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps](https://arxiv.org/pdf/1312.6034.pdf)|2745|[Pytorch](https://github.com/huanghao-code/VisCNN_ICLR_2014_Saliency)|
|2013|ICCV|[Hoggles: Visualizing object detection features](https://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Vondrick_HOGgles_Visualizing_Object_2013_ICCV_paper.pdf)|301|
 
+ [ ] 论文talk
